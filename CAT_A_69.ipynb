{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKkU0U1wM7Bp"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "Our system is designed to recommend the best financial advisor (agent) for a new client by combining several techniques. In essence, we use three different “views” of the data, then combine them with a final machine learning model. The three parts are:\n",
    "\n",
    "1. **Metric Learning (Client Similarity)**\n",
    "2. **Agent–Product Rating**\n",
    "3. **Agent–Client Background Similarity**\n",
    "\n",
    "Finally, we merge all these features and train an XGBoost classifier to output a final ranking score for each agent.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Metric Learning (Client Similarity)\n",
    "\n",
    "- **Goal:** Learn a way to measure how similar two clients are.\n",
    "- **How:**  \n",
    "  - We preprocessed all our historical client data (using one‑hot encoding for categorical data and scaling for numerical data).\n",
    "  - We then built positive and negative pairs of clients:\n",
    "    - **Positive pair:** Two clients who have been served by the same agent.\n",
    "    - **Negative pair:** Two clients who have never been served by the same agent.\n",
    "  - Using these pairs, we trained a **Siamese neural network**.  \n",
    "    - This network learns to embed (or convert) each client into a 16-dimensional vector where similar clients are close together.\n",
    "  - Finally, we precompute the embeddings for all historical clients so that for a new client we can quickly find the most similar ones.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Agent–Product Rating\n",
    "\n",
    "- **Goal:** Identify which financial products are most suitable for the new client, then measure how experienced each agent is with those products.\n",
    "- **How:**  \n",
    "  - For the new client, we use our Siamese network to find the top similar clients.\n",
    "  - We then look at the policies (sales) of those similar clients to see which products were most frequently purchased. This gives us a **suitability score** for each product.\n",
    "  - Next, for each agent we look at their historical product conversion rates and product expertise (which we turned into dummy variables).  \n",
    "    - For each top product, we calculate a score based on a weighted combination of the agent’s conversion rate and expertise.\n",
    "  - We sum these scores across the top products to get an overall **agent–product rating**.  \n",
    "  - Finally, we normalize these ratings.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Agent–Client Background Similarity\n",
    "\n",
    "- **Goal:** Measure how similar an agent's typical client profile is to our new client using basic demographics.\n",
    "- **How:**  \n",
    "  - We use only three features from both sides: age, gender, and marital status.\n",
    "  - For agents, we use their own demographic information (e.g., agent_age, agent_female, and agent_marital).\n",
    "  - For the client, we use their age, client_female, and client_marital.\n",
    "  - We compute the cosine similarity between the new client’s demographic vector and each agent’s demographic vector.  \n",
    "    - A higher cosine similarity means the agent is more experienced with a demographic profile similar to the new client.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Final Model & Ranking\n",
    "\n",
    "- **Goal:** Combine the above three components into a single score that tells us which agent is most likely to succeed with the new client.\n",
    "- **How:**  \n",
    "  - We merge features from the agent–client similarity (like match_count, gender alignment, and age alignment), the agent–product rating, and the background similarity scores.\n",
    "  - We define a binary target for training: if an agent has ever served a client similar to the new client, that’s a success (target=1); otherwise, it's 0.\n",
    "  - We split our data into 80% training and 20% testing.\n",
    "  - Then, we train an XGBoost classifier (a powerful gradient boosting model) on these features to predict the likelihood of a successful match.\n",
    "  - The model outputs a probability score (final_score) for each agent. Agents are ranked based on this final_score.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Metric Learning:** Finds a compact representation of client data so we can identify similar clients.\n",
    "- **Agent–Product Rating:** Determines which products are most suitable for the new client and measures each agent’s expertise and success with those products.\n",
    "- **Background Similarity:** Compares the new client’s demographics to the agent’s typical client profile.\n",
    "- **Final Ranking:** Combines all of the above with a machine learning model (XGBoost) to rank agents.\n",
    "\n",
    "This multi-step approach allows us to incorporate both behavioral (what products are bought, which agents have success) and demographic information to provide a robust recommendation for matching a new client with the best advisor.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAN3P0MVM7Bs"
   },
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15014,
     "status": "ok",
     "timestamp": 1739590886759,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "zPWYrmfXM7Bt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, roc_curve\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu_eSGCgM7Bu"
   },
   "source": [
    "### Agent Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 974,
     "status": "error",
     "timestamp": 1739590887722,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "sOMAliP9M7Bu",
    "outputId": "cbdf3942-ac1f-4643-ba3e-6374d05ad787"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# 1. Load the Agent Data\n",
    "############################################\n",
    "df_agents = pd.read_parquet('nus_agent_info_df.parquet')\n",
    "\n",
    "############################################\n",
    "# 2. Create \"agent_female\" Column\n",
    "############################################\n",
    "# Here, we assume that the 'agent_gender' column contains values like \"F\", \"Female\", \"M\", \"Male\", etc.\n",
    "# We define agent_female = 1 if the gender (after stripping and converting to uppercase) starts with 'F'; else 0.\n",
    "df_agents['agent_female'] = df_agents['agent_gender'].apply(lambda x: 1 if isinstance(x, str) and x.strip().upper().startswith('F') else 0)\n",
    "\n",
    "############################################\n",
    "# 3. Drop Unnecessary Columns\n",
    "############################################\n",
    "df_agents = df_agents.drop(columns=['agent_gender', 'pct_SX0_unknown', 'cluster'])\n",
    "\n",
    "############################################\n",
    "# 4. Convert \"agent_product_expertise\" into Dummy Variables\n",
    "############################################\n",
    "# The 'agent_product_expertise' column contains lists (or string representations of lists) of products.\n",
    "# First, define a helper to parse each value into a list.\n",
    "def parse_expertise(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    elif isinstance(x, list):\n",
    "        return x\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "df_agents['agent_product_expertise'] = df_agents['agent_product_expertise'].apply(parse_expertise)\n",
    "\n",
    "# Use MultiLabelBinarizer to create dummy columns.\n",
    "mlb = MultiLabelBinarizer()\n",
    "expertise_array = mlb.fit_transform(df_agents['agent_product_expertise'])\n",
    "expertise_dummies = pd.DataFrame(\n",
    "    expertise_array,\n",
    "    columns=[f\"agent_expertise_{p}\" for p in mlb.classes_],\n",
    "    index=df_agents.index\n",
    ")\n",
    "\n",
    "# Define the desired products (subset) as given.\n",
    "desired_products = ['prod_0', 'prod_2', 'prod_4', 'prod_6', 'prod_7', 'prod_8', 'prod_9']\n",
    "desired_columns = [f\"agent_expertise_{p}\" for p in desired_products]\n",
    "# Reindex to ensure all desired columns exist (fill missing with zeros).\n",
    "expertise_dummies = expertise_dummies.reindex(columns=desired_columns, fill_value=0)\n",
    "\n",
    "# Concatenate the dummies with the original DataFrame and drop the original expertise column.\n",
    "df_agents = pd.concat([df_agents, expertise_dummies], axis=1)\n",
    "df_agents = df_agents.drop(columns=['agent_product_expertise'])\n",
    "\n",
    "############################################\n",
    "# 5. Check Age Group Percentage Columns\n",
    "############################################\n",
    "age_group_cols = ['pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29',\n",
    "                  'pct_AG04_30to34', 'pct_AG05_35to39', 'pct_AG06_40to44',\n",
    "                  'pct_AG07_45to49', 'pct_AG08_50to54', 'pct_AG09_55to59',\n",
    "                  'pct_AG10_60up']\n",
    "\n",
    "# Calculate the row sums.\n",
    "row_sums = df_agents[age_group_cols].sum(axis=1)\n",
    "# If the sums are not approximately 1 (within a tolerance), print a warning.\n",
    "if not np.allclose(row_sums, 1, atol=0.01):\n",
    "    print(\"Warning: Not all rows in the age group columns sum to 1.\")\n",
    "    # Optionally, you could normalize these rows. For example, uncomment the following:\n",
    "    # df_agents[age_group_cols] = df_agents[age_group_cols].div(row_sums, axis=0)\n",
    "else:\n",
    "    print(\"All rows in age group columns sum to 1 within tolerance.\")\n",
    "\n",
    "############################################\n",
    "# 6. Change 'agent_marital' to Full Names\n",
    "############################################\n",
    "# Suppose the current 'agent_marital' column contains abbreviated values:\n",
    "# 'M' for Married, 'S' for Single, 'U' for Unknown, 'D' for Divorced, 'W' for Widowed.\n",
    "marital_mapping = {\n",
    "    'M': 'Married',\n",
    "    'S': 'Single',\n",
    "    'U': 'Unknown',\n",
    "    'D': 'Divorced',\n",
    "    'W': 'Widowed'\n",
    "}\n",
    "df_agents['agent_marital'] = df_agents['agent_marital'].map(marital_mapping)\n",
    "\n",
    "############################################\n",
    "# 7. Check the Final DataFrame\n",
    "############################################\n",
    "df_agents.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlOyUt0ZM7Bv"
   },
   "source": [
    "### Client Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887723,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "Uooj1rcTM7Bv"
   },
   "outputs": [],
   "source": [
    "df_clients = pd.read_parquet('nus_client_info_df.parquet')\n",
    "# Change Data Types\n",
    "df_clients['family_size'] = df_clients['family_size'].astype('Int64')\n",
    "df_clients['economic_status'] = df_clients['economic_status'].astype('Int64')\n",
    "df_clients['household_size'] = df_clients['household_size'].astype('Int64')\n",
    "# Remove invalid postal code data\n",
    "df_clients = df_clients[df_clients['cltpcode'] <= '900000']\n",
    "df_clients = df_clients.dropna(subset=['cltdob', 'race_desc_map'])\n",
    "df_clients['age'] = 2025 - df_clients['cltdob'].dt.year\n",
    "df_clients = df_clients.dropna(subset=['economic_status'])\n",
    "df_clients = df_clients.rename(columns={'secuityno': 'securityno'})\n",
    "df_clients.loc[df_clients['securityno'] == 'CIN:11715', 'household_size_grp'] = 'HH3_80to100'\n",
    "df_clients.loc[df_clients['securityno'] == 'CIN:11715', 'family_size_grp'] = 'FS4_60to80'\n",
    "income_mapping = {\n",
    "    'HH1_lt40': 1,\n",
    "    'HH2_40to80': 2,\n",
    "    'HH3_80to100': 3,\n",
    "    'HH4_100to120': 4,\n",
    "    'HH5_120up': 5\n",
    "}\n",
    "\n",
    "df_clients['household_size_grp'] = df_clients['household_size_grp'].map(income_mapping)\n",
    "# Map family size groups\n",
    "\n",
    "family_mapping = {\n",
    "    'FS1_lt20': 1,\n",
    "    'FS2_20to40': 2,\n",
    "    'FS3_40to60': 3,\n",
    "    'FS4_60to80': 4,\n",
    "    'FS5_80up': 5\n",
    "}\n",
    "\n",
    "\n",
    "df_clients['family_size_grp'] = df_clients['family_size_grp'].map(family_mapping)\n",
    "# Remove customer with marital status as P\n",
    "df_clients = df_clients[df_clients['marryd'] != 'P']\n",
    "# Remove columns\n",
    "df_clients.drop(['cltdob', 'race_desc_map'], axis=1, inplace=True)\n",
    "\n",
    "df_clients['client_marital'] = df_clients['marryd'].map(marital_mapping)\n",
    "# Map postal codes\n",
    "postal_districts = {'01': '01', '02': '01', '03': '01', '04': '01', '05': '01', '06': '01', '07': '02', '08': '02', '14': '03', '15': '03', '16': '03', '09': '04', '10': '04', '11': '05', '12': '05', '13': '05', '17': '06', '18': '07', '19': '07', '20': '08', '21': '08', '22': '09', '23': '09', '24': '10', '25': '10', '26': '10', '27': '10', '28': '11', '29': '11', '30': '11', '31': '12', '32': '12', '33': '12', '34': '13', '35': '13', '36': '13', '37': '13', '38': '14', '39': '14', '40': '14', '41': '14', '42': '15', '43': '15', '44': '15', '45': '15', '46': '16', '47': '16', '48': '16', '49': '17', '50': '17', '81': '17', '51': '18', '52': '18', '53': '19', '54': '19', '55': '19', '82': '19', '56': '20', '57': '20', '58': '21', '59': '21', '60': '22', '61': '22', '62': '22', '63': '22', '64': '22', '65': '23', '66': '23', '67': '23', '68': '23', '69': '24', '70': '24', '71': '24', '72': '25', '73': '25', '77': '26', '78': '26', '75': '27', '76': '27', '79': '28', '80': '28'}\n",
    "\n",
    "# # Convert the postal code to an integer, then extract the first two digits\n",
    "# df_clients['cltpcode'] = df_clients['cltpcode'].astype(int)\n",
    "\n",
    "# # Extract the first two digits by integer division\n",
    "# df_clients['district_code'] = df_clients['cltpcode'] // 10000\n",
    "# df_clients['district_code'] = df_clients['district_code'].astype(str)\n",
    "\n",
    "# # Map the district code to the corresponding district\n",
    "# df_clients['district'] = df_clients['district_code'].map(postal_districts)\n",
    "df_clients['district'] = df_clients['cltpcode'].str[:2].map(postal_districts)\n",
    "df_clients['client_female'] = df_clients['cltsex'].apply(lambda x: 1 if isinstance(x, str) and x.strip().upper().startswith('F') else 0)\n",
    "# Remove columns\n",
    "df_clients.drop(['cltsex', 'marryd', 'cltpcode','household_size_grp' , 'family_size_grp'], axis=1, inplace=True)\n",
    "df_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHWNiwKuM7Bw"
   },
   "source": [
    "### Policy Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887723,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "6NZRyysMM7Bw"
   },
   "outputs": [],
   "source": [
    "# Load the policy data from a parquet file.\n",
    "df_policies = pd.read_parquet(\"nus_policy_info_df.parquet\")\n",
    "\n",
    "# Ensure that 'occdate' is in datetime format.\n",
    "df_policies['occdate'] = pd.to_datetime(df_policies['occdate'])\n",
    "\n",
    "# Create a new column 'policy_age' as the difference between 2025 and the year of the occdate.\n",
    "df_policies['policy_age'] = 2025 - df_policies['occdate'].dt.year\n",
    "\n",
    "# Drop the unnecessary columns.\n",
    "df_policies = df_policies.drop(columns=['occdate', 'flg_main', 'flg_rider', 'flg_inforce', 'flg_cancel', 'flg_converted', 'product_grp'])\n",
    "df_policies.rename(columns={'secuityno': 'securityno'}, inplace=True)\n",
    "\n",
    "# Optionally, inspect the resulting DataFrame.\n",
    "df_policies.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sap5yWXCM7Bw"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887723,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "wZoeXVXsM7Bw"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# SETUP & NEW CLIENT DATA\n",
    "############################################\n",
    "new_client_data = {\n",
    "    'securityno': 'NEW:001',\n",
    "    'client_marital': 'Married',\n",
    "    'economic_status': 52,\n",
    "    'household_size': 4,\n",
    "    'family_size': 5,\n",
    "    'age': 37,\n",
    "    'district': 25,\n",
    "    'client_female': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887723,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "y6eBxoAmM7Bw"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# SECTION 1: AGENT–SIMILAR CLIENTS (METRIC LEARNING)\n",
    "############################################\n",
    "\n",
    "# 1.2 Preprocess Client Data\n",
    "\n",
    "# Define categorical and numerical columns for feature processing\n",
    "categorical_cols = ['client_marital', 'district']\n",
    "numerical_cols = ['economic_status', 'age', 'client_female', 'household_size', 'family_size']\n",
    "\n",
    "# Convert categorical variables into one-hot encoded features\n",
    "df_client_encoded = pd.get_dummies(df_clients, columns=categorical_cols)\n",
    "\n",
    "# Standardize (scale) numerical columns to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "df_client_encoded[numerical_cols] = scaler.fit_transform(df_client_encoded[numerical_cols])\n",
    "\n",
    "# Extract feature column names, excluding the unique client identifier ('securityno')\n",
    "client_feature_cols = [col for col in df_client_encoded.columns if col != 'securityno']\n",
    "\n",
    "# Convert client data into a dictionary format where each client's features are stored under their security number\n",
    "client_features = df_client_encoded.set_index('securityno')[client_feature_cols].to_dict('index')\n",
    "\n",
    "# 1.3 Build Pairs for Metric Learning\n",
    "\n",
    "# Filter policies to include only clients present in the processed dataset\n",
    "df_policies_filtered = df_policies[df_policies['securityno'].isin(df_client_encoded['securityno'])]\n",
    "\n",
    "# Create a dictionary that maps each client to the set of agents they have worked with\n",
    "client_agents = df_policies_filtered.groupby('securityno')['agntnum'].apply(set).to_dict()\n",
    "\n",
    "# Create a set to store positive pairs (clients who were served by the same agent)\n",
    "positive_pairs = set()\n",
    "\n",
    "# Iterate over each agent and find all unique client pairs that the agent has served\n",
    "for agent, group in df_policies_filtered.groupby('agntnum'):\n",
    "    clients = group['securityno'].unique()\n",
    "    clients = [c for c in clients if c in client_features]  # Ensure clients are in the dataset\n",
    "    for pair in combinations(clients, 2):  # Create all possible client pairs\n",
    "        positive_pairs.add(tuple(sorted(pair)))  # Store the pair in a sorted format\n",
    "\n",
    "# Get a list of all client IDs\n",
    "all_clients = list(client_features.keys())\n",
    "\n",
    "# Generate the same number of negative pairs (clients who never shared an agent)\n",
    "num_negatives = len(positive_pairs)\n",
    "negative_pairs = set()\n",
    "\n",
    "# Randomly create negative pairs until we have the same number as positive pairs\n",
    "while len(negative_pairs) < num_negatives:\n",
    "    c1, c2 = np.random.choice(all_clients, 2, replace=False)  # Select two random clients\n",
    "    pair = tuple(sorted((c1, c2)))  # Ensure consistent ordering\n",
    "    agents1 = client_agents.get(c1, set())  # Get agents who served client 1\n",
    "    agents2 = client_agents.get(c2, set())  # Get agents who served client 2\n",
    "\n",
    "    # Ensure the two clients have never been served by the same agent\n",
    "    if len(agents1.intersection(agents2)) == 0:\n",
    "        negative_pairs.add(pair)\n",
    "\n",
    "# Combine positive and negative pairs into a single dataset\n",
    "pairs = list(positive_pairs) + list(negative_pairs)\n",
    "\n",
    "# Assign labels: 1 for positive pairs (similar clients), 0 for negative pairs (dissimilar clients)\n",
    "labels = [1]*len(positive_pairs) + [0]*len(negative_pairs)\n",
    "\n",
    "# Print the number of pairs created\n",
    "print(f\"Number of pairs: {len(pairs)} (Positive: {len(positive_pairs)}, Negative: {len(negative_pairs)})\")\n",
    "\n",
    "# Function to extract client features given a client ID\n",
    "def get_feature(securityno):\n",
    "    return np.array(list(client_features[securityno].values()))\n",
    "\n",
    "# Create input arrays for model training (features for each client in the pair)\n",
    "X1 = np.array([get_feature(c1) for (c1, c2) in pairs])\n",
    "X2 = np.array([get_feature(c2) for (c1, c2) in pairs])\n",
    "y = np.array(labels)  # Labels for similarity classification\n",
    "\n",
    "# 1.4 Build the Siamese Network\n",
    "\n",
    "# Define the input dimension based on the number of client features\n",
    "input_dim = X1.shape[1]\n",
    "\n",
    "# Function to create the base neural network for feature embedding\n",
    "def create_base_network(input_dim):\n",
    "    inp = Input(shape=(input_dim,))\n",
    "    x = Dense(128, activation='relu')(inp)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)  # Final embedding size is 16-dimensional\n",
    "    return Model(inp, x)\n",
    "\n",
    "# Create the base network\n",
    "base_network = create_base_network(input_dim)\n",
    "\n",
    "# Define inputs for the Siamese network (two clients to compare)\n",
    "input_a = Input(shape=(input_dim,))\n",
    "input_b = Input(shape=(input_dim,))\n",
    "\n",
    "# Process both inputs using the same network (shared weights)\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Function to compute Euclidean distance between two client embeddings\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "# Compute the Euclidean distance between the two processed inputs\n",
    "distance = Lambda(euclidean_distance, output_shape=lambda shapes: (shapes[0][0], 1))([processed_a, processed_b])\n",
    "\n",
    "# Create the Siamese network model\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# Define contrastive loss function for training\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1  # Defines the decision boundary\n",
    "    square_pred = K.square(y_pred)  # Squared distance for similar pairs\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))  # Distance for dissimilar pairs\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)  # Compute loss\n",
    "\n",
    "# Compile the model with contrastive loss and Adam optimizer\n",
    "model.compile(loss=contrastive_loss, optimizer='adam')\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model using labeled client pairs\n",
    "model.fit([X1, X2], y, batch_size=128, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Generate embeddings for all clients in the dataset using the trained model\n",
    "client_ids = df_client_encoded['securityno'].tolist()\n",
    "client_data = df_client_encoded[client_feature_cols].astype('float32').values\n",
    "client_embeddings = base_network.predict(client_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887723,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "p3xy37vUM7Bx"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# SECTION 2: AGENT RECOMMENDATION (AGENT–PRODUCT & AGENT–CLIENT)\n",
    "############################################\n",
    "def preprocess_new_client(new_client_data):\n",
    "    processed_client = {}\n",
    "    processed_client['economic_status'] = (new_client_data['economic_status'] - scaler.mean_[0]) / scaler.scale_[0]\n",
    "    processed_client['age'] = (new_client_data['age'] - scaler.mean_[1]) / scaler.scale_[1]\n",
    "    processed_client['client_female'] = (new_client_data['client_female'] - scaler.mean_[2]) / scaler.scale_[2]\n",
    "    processed_client['household_size'] = (new_client_data['household_size'] - scaler.mean_[3]) / scaler.scale_[3]\n",
    "    processed_client['family_size'] = (new_client_data['family_size'] - scaler.mean_[4]) / scaler.scale_[4]\n",
    "    for col in categorical_cols:\n",
    "        for category in df_client_encoded.columns:\n",
    "            if category.startswith(col + '_'):\n",
    "                processed_client[category] = 1 if category.endswith(str(new_client_data[col])) else 0\n",
    "    return np.array([processed_client[col] for col in client_feature_cols])\n",
    "def get_top_similar_clients(new_client_vector, top_n=50):\n",
    "    new_client_embedding = base_network.predict(np.array([new_client_vector]))\n",
    "    sim_scores = cosine_similarity(new_client_embedding, client_embeddings)[0]\n",
    "    top_indices = np.argsort(-sim_scores)[:top_n]\n",
    "    top_similar_clients = [client_ids[i] for i in top_indices]\n",
    "    return top_similar_clients\n",
    "new_client_vector = preprocess_new_client(new_client_data)\n",
    "top_similar_clients = get_top_similar_clients(new_client_vector, top_n=50)\n",
    "agent_success = (\n",
    "    df_policies[df_policies['securityno'].isin(top_similar_clients)]\n",
    "    .groupby('agntnum')\n",
    "    .size()\n",
    "    .reset_index(name='match_count')\n",
    ")\n",
    "df_agents_subset = df_agents[['agntnum',\n",
    "                              'pct_SX1_male', 'pct_SX2_female',\n",
    "                              'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29',\n",
    "                              'pct_AG04_30to34', 'pct_AG05_35to39', 'pct_AG06_40to44',\n",
    "                              'pct_AG07_45to49', 'pct_AG08_50to54', 'pct_AG09_55to59','pct_AG10_60up']]\n",
    "df_train_agents = df_agents_subset.merge(agent_success, on='agntnum', how='left').copy()\n",
    "df_train_agents['match_count'] = df_train_agents['match_count'].fillna(0)\n",
    "if new_client_data.get('client_female', 0) == 1:\n",
    "    df_train_agents['gender_score'] = df_train_agents['pct_SX2_female']\n",
    "else:\n",
    "    df_train_agents['gender_score'] = df_train_agents['pct_SX1_male']\n",
    "age = new_client_data.get('age')\n",
    "if age < 20:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG01_lt20']\n",
    "elif age < 25:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG02_20to24']\n",
    "elif age < 30:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG03_25to29']\n",
    "elif age < 35:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG04_30to34']\n",
    "elif age < 40:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG05_35to39']\n",
    "elif age < 45:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG06_40to44']\n",
    "elif age < 50:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG07_45to49']\n",
    "elif age < 55:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG08_50to54']\n",
    "elif age < 60:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG09_55to59']\n",
    "else:\n",
    "    df_train_agents['age_score'] = df_train_agents['pct_AG10_60up']\n",
    "\n",
    "df_agents2 = df_agents[['agntnum','agent_tenure',\n",
    "       'cnt_converted', 'annual_premium_cnvrt', 'pct_lapsed', 'pct_cancel',\n",
    "       'pct_inforce', 'pct_prod_1_cnvrt', 'pct_prod_2_cnvrt',\n",
    "       'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt', 'pct_prod_5_cnvrt',\n",
    "       'pct_prod_6_cnvrt', 'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt',\n",
    "       'pct_prod_9_cnvrt','agent_expertise_prod_0', 'agent_expertise_prod_6',\n",
    "       'agent_expertise_prod_4', 'agent_expertise_prod_9',\n",
    "       'agent_expertise_prod_2', 'agent_expertise_prod_7',\n",
    "       'agent_expertise_prod_8']].copy()\n",
    "df_policies2 = df_policies[['chdrnum', 'agntnum', 'securityno', 'product',\n",
    "       'flg_lapsed', 'flg_expire', 'cust_age_at_purchase_grp',\n",
    "       'cust_tenure_at_purchase_grp']].copy()\n",
    "available_products = {0, 2, 4, 6, 7, 8, 9}\n",
    "if df_policies2['product'].dtype == object:\n",
    "    df_policies2['product'] = df_policies2['product'].str.replace('prod_', '', regex=False).astype(int)\n",
    "\n",
    "def get_top_similar_clients_with_scores(new_client_vector, top_n=50):\n",
    "    new_client_embedding = base_network.predict(np.array([new_client_vector]))\n",
    "    sim_scores = cosine_similarity(new_client_embedding, client_embeddings)[0]\n",
    "    top_indices = np.argsort(-sim_scores)[:top_n]\n",
    "    top_similar_clients = [client_ids[i] for i in top_indices]\n",
    "    top_similar_scores = [sim_scores[i] for i in top_indices]\n",
    "    return top_similar_clients, top_similar_scores\n",
    "\n",
    "def get_top_k_product_suitability(new_client_data, k=7):\n",
    "    new_client_encoded = preprocess_new_client(new_client_data)\n",
    "    new_client_vector = new_client_encoded.astype(np.float32)\n",
    "    print(\"New client vector shape:\", new_client_vector.shape)\n",
    "    top_clients, top_scores = get_top_similar_clients_with_scores(new_client_vector, top_n=50)\n",
    "    weight_map = dict(zip(top_clients, top_scores))\n",
    "    df_sub = df_policies2[df_policies2['securityno'].isin(top_clients)]\n",
    "    df_sub = df_sub[df_sub['product'].isin(available_products)].copy()\n",
    "    df_sub['sim_weight'] = df_sub['securityno'].map(weight_map).fillna(0)\n",
    "    product_agg = df_sub.groupby('product')['sim_weight'].sum().reset_index()\n",
    "    total_weight = product_agg['sim_weight'].sum()\n",
    "    if total_weight > 0:\n",
    "        product_agg['suitability'] = product_agg['sim_weight'] / total_weight\n",
    "    else:\n",
    "        product_agg['suitability'] = 0\n",
    "    product_agg = product_agg.sort_values('suitability', ascending=False)\n",
    "    top_products = product_agg.head(k)['product'].tolist()\n",
    "    suitability_dict = product_agg.set_index('product')['suitability'].to_dict()\n",
    "    print(\"DEBUG: Top products:\", top_products)\n",
    "    print(\"DEBUG: Suitability dict:\", suitability_dict)\n",
    "    return top_products, suitability_dict\n",
    "\n",
    "top_products, suitability_dict = get_top_k_product_suitability(new_client_data, k=3)\n",
    "print(\"Final Top Products for New Client:\", top_products)\n",
    "print(\"Final Suitability Scores:\", suitability_dict)\n",
    "\n",
    "if 'pct_prod_0_cnvrt' not in df_agents2.columns:\n",
    "    df_agents2 = df_agents2.copy()\n",
    "    df_agents2['pct_prod_0_cnvrt'] = 1 - df_agents2[['pct_prod_1_cnvrt', 'pct_prod_2_cnvrt',\n",
    "                                                     'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt',\n",
    "                                                     'pct_prod_5_cnvrt', 'pct_prod_6_cnvrt',\n",
    "                                                     'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt',\n",
    "                                                     'pct_prod_9_cnvrt']].sum(axis=1)\n",
    "    df_agents2['pct_prod_0_cnvrt'] = df_agents2['pct_prod_0_cnvrt'].clip(lower=0)\n",
    "\n",
    "conv_map = {\n",
    "    0: 'pct_prod_0_cnvrt',\n",
    "    1: 'pct_prod_1_cnvrt',\n",
    "    2: 'pct_prod_2_cnvrt',\n",
    "    3: 'pct_prod_3_cnvrt',\n",
    "    4: 'pct_prod_4_cnvrt',\n",
    "    5: 'pct_prod_5_cnvrt',\n",
    "    6: 'pct_prod_6_cnvrt',\n",
    "    7: 'pct_prod_7_cnvrt',\n",
    "    8: 'pct_prod_8_cnvrt',\n",
    "    9: 'pct_prod_9_cnvrt'\n",
    "}\n",
    "\n",
    "exp_map = {\n",
    "    0: 'agent_expertise_prod_0',\n",
    "    2: 'agent_expertise_prod_2',\n",
    "    4: 'agent_expertise_prod_4',\n",
    "    6: 'agent_expertise_prod_6',\n",
    "    7: 'agent_expertise_prod_7',\n",
    "    8: 'agent_expertise_prod_8',\n",
    "    9: 'agent_expertise_prod_9'\n",
    "}\n",
    "\n",
    "def compute_agent_product_rating(new_client_data, k=3, w_exp=0.1, w_conv=0.9):\n",
    "    top_products, suitability_dict = get_top_k_product_suitability(new_client_data, k=k)\n",
    "    print(\"Top suitable products:\", top_products)\n",
    "    print(\"Suitability scores:\", suitability_dict)\n",
    "\n",
    "    def agent_rating(row):\n",
    "        total = 0.0\n",
    "        for p in top_products:\n",
    "            s = suitability_dict.get(p, 0)\n",
    "            conv_col = conv_map.get(p)\n",
    "            conv = row[conv_col] if conv_col in row.index else 0\n",
    "            exp_col = exp_map.get(p)\n",
    "            exp = row[exp_col] if (exp_col is not None and exp_col in row.index) else 0\n",
    "            total += s * (w_exp * exp + w_conv * conv)\n",
    "        return total\n",
    "\n",
    "    df_agents2_cp = df_agents2.copy()\n",
    "    df_agents2_cp['agent_product_rating'] = df_agents2_cp.apply(agent_rating, axis=1)\n",
    "\n",
    "    min_rating = df_agents2_cp['agent_product_rating'].min()\n",
    "    max_rating = df_agents2_cp['agent_product_rating'].max()\n",
    "    if max_rating - min_rating > 0:\n",
    "        df_agents2_cp['agent_product_rating_norm'] = (df_agents2_cp['agent_product_rating'] - min_rating) / (max_rating - min_rating)\n",
    "    else:\n",
    "        df_agents2_cp['agent_product_rating_norm'] = 0.0\n",
    "\n",
    "    df_result = df_agents2_cp[['agntnum', 'agent_product_rating', 'agent_product_rating_norm']].sort_values(by='agent_product_rating_norm', ascending=False)\n",
    "    return df_result\n",
    "\n",
    "df_agent_ratings = compute_agent_product_rating(new_client_data, k=10)\n",
    "print(\"Top agents based on product experience rating:\")\n",
    "print(df_agent_ratings.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887723,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "gdeLdKi8M7By"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# SECTION 4: AGENT-CLIENT BACKGROUND SIMILARITY\n",
    "############################################\n",
    "\n",
    "marital_map = {'Married': 3, 'Single': 2, 'Divorced': 1, 'Unknown': 0}\n",
    "\n",
    "df_agents_new = df_agents.copy()\n",
    "df_clients_new = df_clients.copy()\n",
    "\n",
    "df_agents_new['agent_marital'] = df_agents_new['agent_marital'].map(marital_map)\n",
    "df_clients_new['client_marital'] = df_clients_new['client_marital'].map(marital_map)\n",
    "\n",
    "df_agents_new = df_agents_new.dropna(subset=['agntnum', 'agent_age', 'agent_marital', 'agent_female'])\n",
    "df_clients_new = df_clients_new.dropna(subset=['securityno', 'client_marital', 'age', 'client_female'])\n",
    "\n",
    "def create_agent_vector(agent_row):\n",
    "    return np.array([agent_row['agent_age'], agent_row['agent_female'], agent_row['agent_marital']], dtype=float)\n",
    "\n",
    "def create_client_vector(client_row):\n",
    "    return np.array([client_row['age'], client_row['client_female'], client_row['client_marital']], dtype=float)\n",
    "\n",
    "def compute_agent_client_similarity(new_client_data, df_agents):\n",
    "    new_client_series = pd.Series(new_client_data)\n",
    "    if isinstance(new_client_series['client_marital'], str):\n",
    "        new_client_series['client_marital'] = marital_map.get(new_client_series['client_marital'], 0)\n",
    "    new_client_vector = create_client_vector(new_client_series)\n",
    "    similarities = []\n",
    "    for idx, agent_row in df_agents.iterrows():\n",
    "        agent_vector = create_agent_vector(agent_row)\n",
    "        norm_client = np.linalg.norm(new_client_vector)\n",
    "        norm_agent = np.linalg.norm(agent_vector)\n",
    "        if norm_client == 0 or norm_agent == 0:\n",
    "            sim = 0\n",
    "        else:\n",
    "            sim = np.dot(new_client_vector, agent_vector) / (norm_client * norm_agent)\n",
    "        similarities.append(sim)\n",
    "    df_agents_sim = df_agents.copy()\n",
    "    df_agents_sim['background_similarity'] = similarities\n",
    "    df_agents_sim = df_agents_sim.sort_values(by='background_similarity', ascending=False)\n",
    "    return df_agents_sim[['agntnum','background_similarity']]\n",
    "\n",
    "df_agent_similarity = compute_agent_client_similarity(new_client_data, df_agents_new)\n",
    "print(\"Top agents based on background similarity:\")\n",
    "print(df_agent_similarity.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1739590887724,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "25pGJYbdM7By"
   },
   "outputs": [],
   "source": [
    "if 'pct_prod_0_cnvrt' not in df_agents2.columns:\n",
    "    df_agents2 = df_agents2.copy()\n",
    "    df_agents2['pct_prod_0_cnvrt'] = 1 - df_agents2[['pct_prod_1_cnvrt', 'pct_prod_2_cnvrt',\n",
    "                                                     'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt',\n",
    "                                                     'pct_prod_5_cnvrt', 'pct_prod_6_cnvrt',\n",
    "                                                     'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt',\n",
    "                                                     'pct_prod_9_cnvrt']].sum(axis=1)\n",
    "    df_agents2['pct_prod_0_cnvrt'] = df_agents2['pct_prod_0_cnvrt'].clip(lower=0)\n",
    "\n",
    "conv_map = {\n",
    "    0: 'pct_prod_0_cnvrt',\n",
    "    1: 'pct_prod_1_cnvrt',\n",
    "    2: 'pct_prod_2_cnvrt',\n",
    "    3: 'pct_prod_3_cnvrt',\n",
    "    4: 'pct_prod_4_cnvrt',\n",
    "    5: 'pct_prod_5_cnvrt',\n",
    "    6: 'pct_prod_6_cnvrt',\n",
    "    7: 'pct_prod_7_cnvrt',\n",
    "    8: 'pct_prod_8_cnvrt',\n",
    "    9: 'pct_prod_9_cnvrt'\n",
    "}\n",
    "\n",
    "exp_map = {\n",
    "    0: 'agent_expertise_prod_0',\n",
    "    2: 'agent_expertise_prod_2',\n",
    "    4: 'agent_expertise_prod_4',\n",
    "    6: 'agent_expertise_prod_6',\n",
    "    7: 'agent_expertise_prod_7',\n",
    "    8: 'agent_expertise_prod_8',\n",
    "    9: 'agent_expertise_prod_9'\n",
    "}\n",
    "\n",
    "def compute_agent_product_rating(new_client_data, k=3, w_exp=0.1, w_conv=0.9):\n",
    "    top_products, suitability_dict = get_top_k_product_suitability(new_client_data, k=k)\n",
    "    print(\"Top suitable products:\", top_products)\n",
    "    print(\"Suitability scores:\", suitability_dict)\n",
    "\n",
    "    def agent_rating(row):\n",
    "        total = 0.0\n",
    "        for p in top_products:\n",
    "            s = suitability_dict.get(p, 0)\n",
    "            conv_col = conv_map.get(p)\n",
    "            conv = row[conv_col] if conv_col in row.index else 0\n",
    "            exp_col = exp_map.get(p)\n",
    "            exp = row[exp_col] if (exp_col is not None and exp_col in row.index) else 0\n",
    "            total += s * (w_exp * exp + w_conv * conv)\n",
    "        return total\n",
    "\n",
    "    df_agents2_cp = df_agents2.copy()\n",
    "    df_agents2_cp['agent_product_rating'] = df_agents2_cp.apply(agent_rating, axis=1)\n",
    "\n",
    "    min_rating = df_agents2_cp['agent_product_rating'].min()\n",
    "    max_rating = df_agents2_cp['agent_product_rating'].max()\n",
    "    if max_rating - min_rating > 0:\n",
    "        df_agents2_cp['agent_product_rating_norm'] = (df_agents2_cp['agent_product_rating'] - min_rating) / (max_rating - min_rating)\n",
    "    else:\n",
    "        df_agents2_cp['agent_product_rating_norm'] = 0.0\n",
    "\n",
    "    df_result = df_agents2_cp[['agntnum', 'agent_product_rating', 'agent_product_rating_norm']].sort_values(by='agent_product_rating_norm', ascending=False)\n",
    "    return df_result\n",
    "\n",
    "df_agent_ratings = compute_agent_product_rating(new_client_data, k=10)\n",
    "print(\"Top agents based on product experience rating:\")\n",
    "print(df_agent_ratings.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1739590887724,
     "user": {
      "displayName": "HUA SHIAO HONG (Xavier)",
      "userId": "15499868967901239338"
     },
     "user_tz": -480
    },
    "id": "INWYROOQM7Bz"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# SECTION 5: FINAL COMBINED MODEL & RANKING\n",
    "############################################\n",
    "\n",
    "# First Approach: Classification Model\n",
    "final_df_classification = df_train_agents.merge(\n",
    "    df_agent_ratings.set_index('agntnum'), on='agntnum', how='outer'\n",
    ").merge(\n",
    "    df_agent_similarity.set_index('agntnum'), on='agntnum', how='outer'\n",
    ")[['agntnum', 'match_count', 'gender_score', 'age_score', 'agent_product_rating', 'background_similarity']]\n",
    "\n",
    "final_df_classification['target'] = (final_df_classification['match_count'] > 0).astype(int)\n",
    "\n",
    "features = ['match_count', 'gender_score', 'age_score', 'agent_product_rating', 'background_similarity']\n",
    "X = final_df_classification[features]\n",
    "y = final_df_classification['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "final_model_classification = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "final_model_classification.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "final_df_classification['final_score_classification'] = final_model_classification.predict_proba(X)[:, 1]\n",
    "\n",
    "# Second Approach: Regression Model\n",
    "final_df_regression = df_train_agents.merge(\n",
    "    df_agent_ratings.set_index('agntnum'), on='agntnum', how='outer'\n",
    ").merge(\n",
    "    df_agent_similarity.set_index('agntnum'), on='agntnum', how='outer'\n",
    ")[['agntnum', 'match_count', 'gender_score', 'age_score', 'agent_product_rating', 'background_similarity']]\n",
    "\n",
    "final_df_regression['target'] = final_df_regression['match_count']\n",
    "features = ['gender_score', 'age_score', 'agent_product_rating', 'background_similarity']\n",
    "X = final_df_regression[features]\n",
    "y = final_df_regression['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "final_model_regression = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "final_model_regression.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "final_df_regression['final_score_regression'] = final_model_regression.predict(X)\n",
    "\n",
    "# Merge both models' scores using agntnum\n",
    "final_df_combined = final_df_classification[['agntnum', 'final_score_classification']].merge(\n",
    "    final_df_regression[['agntnum', 'final_score_regression']], on='agntnum'\n",
    ")\n",
    "\n",
    "# Compute the combined score (sum of both models' scores)\n",
    "final_df_combined['final_score_combined'] = (\n",
    "    final_df_combined['final_score_classification'] + final_df_combined['final_score_regression']\n",
    ")\n",
    "\n",
    "# Sort by the combined final score\n",
    "final_df_combined_sorted = final_df_combined.sort_values('final_score_combined', ascending=False)\n",
    "\n",
    "# Print the top 5 agents based on the combined score\n",
    "print(\"Top 5 Agents Based on Combined Model:\")\n",
    "final_df_combined_sorted.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
